---
title: "Direct Coefficient Recentering and Rescaling"
author: "Doug Hemken"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    keep_tex: TRUE
    highlight: NULL
    toc: TRUE
    toc_depth: 2
  html_document: 
    highlight: NULL
    toc: TRUE
    toc_depth: 2
---

```{r knitrsetup, echo=FALSE, results="hide"}
knitr::opts_chunk$set(comment=NA)
options(digits=3)
```

# Introduction

For analysts working with linear models, recentering and rescaling the variables under analysis
is such a routine task it hardly garners attention.  In fields where there are no natural,
physical units of measurement - education and psychology, to name two - it is common practice
to refer to standardized units of measure.  It is not uncommon to see analysts fit and report
the same model in both the original and standardized units of measurement.

# The Problem: Higher Order Models

For additive models - models with intercepts and slopes of
single variables to polynomial degree one - the analyst can directly transform the
coefficients in the model via a classic formula that appears in most textbooks. 
Consider, for example, the regression model
$$y = \beta_0 + \beta_1x_1 + \beta_2x_2$$
where $x_1$, $x_2$, and $y$ are all continuous variables.  If we transform the data so that
all the variables are centered, the transformed coefficients for this model are 
given by $\beta_0=0$ and $\beta_i=\beta_i$ for $i\ge 1$.
If we further transform the data so that
all variables are standardized, the transformed coefficients for this model are
given by $\beta_0=0$ and
$$\beta_i=\frac{\sigma_{x_i}}{\sigma_y}\beta_i$$

However, once interaction terms and higher order polynomial terms appear in a
model, the classic formula requires centering and rescaling higher order terms
using means and standard deviations of the higher order data vectors, independent
of the rescaling of lower order terms.  This produces coefficients that are difficult
to interpret because they are on different scales.  While this can be useful
for some purposes, such as calculating predicted values and residuals,
standard practice where the
coefficients are to be interpreted is to recalculate the data, then refit the model.  

Available
software calculates standardized coefficients directly using the classic
approach, perhaps as a legacy of the sweep operations of the 1970s. 
Refitting the model to recalculated data has the advantage
that software also produces a variance-covariance matrix appropriate to the
transformed coefficients, and sets up the software for post-estimation operations
with the transformed model.

It seems to be little appreciated that the coefficients for recentered and rescaled
(including standardized) models can be easily calculated directly.

# Direct Transformation
## One Variable
Transforming the coefficients and the variance-covariance matrix of a linear model with a single
continous outcome and a single continuous predictor is straighforward.  Consider the simple model
$$y = b_0 + b_1x$$
or in the usual matrix form
$$Y=X\beta$$

If we wish to recenter our model in terms of $x_\delta=x-\mu_x$, an arbitrarily recentered $x$,
we can do so without calculating the $x_\delta$.
We use a linear transformation
$C$ to map the vector of coefficients $\beta$ to a vector of centered coefficients, $\beta_\delta$.
$$\beta^\delta=C\beta$$
where $C$ takes the form
$$C=\begin{bmatrix}1 & \mu \\ 0 & 1 \end{bmatrix}$$
Rescaling in terms of $x_z=\frac{x_\delta}{\sigma}$ can be done directly with the
linear transformation
$$S=\begin{bmatrix}1 & 0 \\ 0 & \sigma \end{bmatrix}$$
To standardize the coefficients, we recenter, then rescale.  In one step this is
$$Z = S \times C =\begin{bmatrix}1 & \mu \\ 0 & \sigma \end{bmatrix}$$
To fully standardize this model, the final step is to standardize $y$,
$y_z=(y-\mu_y)/\sigma_y$.  This requires adjusting $\beta_0^z$ by $\mu_y$, which in
this simple case leaves $\beta_0^z=0$, and dividing $\beta^z/\sigma_y$.  More complicated
models also require these two final operations for full standardization.  It will
simplify further discussion to drop consideration of this.

### Example: One Variable Recentering
It is worth noting that "recentering" and "rescaling" may be done with any 
arbitrary constants, although
it is perhaps most often done with a sample mean and sample standard deviation.
However, this same approach
would hold for converting a model where $x$ is expressed, for example,
in degrees Fahrenheit
to one expressed in degrees Centrigrade.

```{r ex1-init}
example <- lm(mpg ~ wt, data=mtcars)
```
Here the coefficients are recentered *as if* the $x$ variable `wt` were
recentered to the sample mean.
```{r ex1-direct}
C <- matrix(c(1,0,mean(mtcars$wt),1), ncol=2)
C%*%coef(example)
```
We can check that this agrees with recentering the data, then refitting
the model.
```{r ex1-data}
wtcentered <- mtcars$wt - mean(mtcars$wt)
check <- lm(mpg ~ wtcentered, data=mtcars)
coef(check)
```

We can also use the same recentering matrix to transform the variance-covariance
matrix of the coefficients.
```{r ex1-vcov}
C%*%vcov(example)%*%t(C)
vcov(check)
# check equality
norm(C%*%vcov(example)%*%t(C)-vcov(check), "F")
```

A change of basis for the column space of $X$ induces a change of basis for the column
space of the coefficient vector, and a change of basis for the row and column space
of the variance-covariance matrix.  $C$, $S$, and $Z$ are change of basis transformations.

## Two Variables
Now consider a model with two continuous independent variables and an interaction term,
so the columns of $X$ are $\begin{bmatrix} 1_n &x_1 &x_2 &x_1x_2 \end{bmatrix}$.  We
compose the coefficient change of basis from the two simple transformations as a direct
product [reference:  Haberman?].  Denote
$$C_1=\begin{bmatrix}1 & \mu_1 \\ 0 & 1 \end{bmatrix}$$
$$C_2=\begin{bmatrix}1 & \mu_2 \\ 0 & 1 \end{bmatrix}$$
Then
$$C = C_2 \otimes C_1 = \begin{bmatrix} 1 & \mu_1 &\mu_2 &\mu_2\mu_1 \\
  0 &1 &0 &\mu_2 \\ 0 &0 &1 &\mu_1 \\ 0 &0 &0 &1 \end{bmatrix}$$
The classic standardization formula skips recentering in adjusting the coefficients.

While this is simple to produce in theory, in practice attention must be
given to the column ordering:  to use $C$ we must recognize that the
column space is ordered $\begin{bmatrix} 1_n &x_1 &x_2 &x_1x_2 \end{bmatrix}$,
so the order must match that of the coefficient vector and the 
variance-covariance matrix, perhaps through permutation.

While in general $C_1 \otimes C_2 \neq C_2 \otimes C_1$, for any such operation
there exists a permutation, $P$, such that $C_1 \otimes C_2 = P^T(C_2 \otimes C_1)P$.
As long as we include simple recentering and rescaling matrices for every variable
used in our coefficient terms, up to a final permutation their order does not matter.

### Example:  Two Variable Recentering
In order to build a recentering matrix, then, we need to collect a labelled vector of
recentering constants, and an ordered vector of coefficient terms.

```{r ex2-init}
source("stdParm functions.r")

ex2 <- lm(mpg ~ wt*disp, data=mtcars)        # the base model
x.means <- colMeans(mtcars[,c("wt","disp")]) # recentering constants (means)
b.terms <- names(coef(ex2))                  # coefficients/terms

C <- recentering.matrix(x.means, b.terms)
C
```

This, then, is what we use to produce recentered coefficients, and the accompanying
variance-covariance matrix.

```{r ex2-direct}
C %*% coef(ex2)

C %*% vcov(ex2) %*% C
```

## Example: Two Variable Rescaling

Rescaling is just as easy.  Here,
$$S_1=\begin{bmatrix}1 &0 \\ 0 &\sigma_1 \end{bmatrix}$$
$$S_2=\begin{bmatrix}1 &0 \\ 0 &\sigma_2 \end{bmatrix}$$
Then
$$S = S_2 \otimes S_1 = \begin{bmatrix} 1 &0 &0 &0 \\
  0 &\sigma_1 &0 &0 \\ 0 &0 &\sigma_2 &0 \\ 0 &0 &0 &\sigma_2\sigma_1 \end{bmatrix}$$
While the classic standardization formula takes a similar diagonal form, the rescaling
constants for higher order terms ($>1$) are different.

Rescaling can be useful independent of recentering, for example, to rescale from United States
customary units to SI units where the zero of each scale remains unchanged.

```{r ex3-scale}
# pounds to kilograms, and cubic inches to liters
x.scales <- c(1/453.592, 61.024)
names(x.scales) <- c("wt", "disp")
  
S <- recentering.matrix(x.scales, b.terms, type="scale")
S
```

Check the coefficients:
```{r ex3-check1}
wtkg  <- mtcars$wt*453.592  # 1000 lbs to kg
displ <- mtcars$disp/61.024 # cu.in. to liters

ex3 <- lm(mpg~wtkg*displ, data=mtcars)

ex3coefs <- cbind(S %*% coef(ex2),coef(ex3))
colnames(ex3coefs) <- c("Direct","Data Trans.")
ex3coefs
```

Compare the variance-covariance matrices:
```{r ex3-check2}
norm(S %*% vcov(ex2) %*% S-vcov(ex3), "F")
```

# Interaction Terms, Factorial Models, and Direct Products

To understand the use of the direct product here, we need to consider the
relationship between the *data* space and the *parameter* space of a model.
In all of the models considered here, the column space of the parameters
is a subset of 
the outer product of the columns space of the data, including a column for
the intercept term.

The data are modeled in an outer product of the data column space.  In transforming the
coefficients, we need the same vector space for both columns and rows - the 
Kronecker operator provides this twofold outer product very neatly.

We may consider a model that includes only the mean of the response as a zero-order model,
sometimes called an intercept-only model.
A model with means of several categories, parameterized as a mean and offsets to that mean,
is a model with multiple intercepts (only), and is also a zero-order model.  Classical
ANOVA models are zero-order models.

A model of a response with a continuous variable includes both an intercept and a slope.
This is a first-order model, with a zero-order term and a first-order term.  Adding
categorical variables to the model adds more intercepts, or zero-order terms.  Adding
continuous variables adds more slopes, or first-order terms.  Such additive models
are all first-order models.

An interaction term is formed as the product of two variables.  A product of categorical
variables adds intercepts to the model.  The interaction of a categorical variable and
a continuous variable adds slopes to the model.  In either case, the order of the model
remains the same.  But the interaction formed from the product of two continuous variables
adds a second-order term to a model, a curvature.

A factorial model is formed by adding all the products of all the zero- and first-order
terms, in all combinations.  If we think of the terms in a model as its column space, then
any linear model resides in a subspace of the factorial column space.  The columns of any 
linear model are a subset of the columns in a full-factorial model.

Finally, transformations of the parameters are measured within the parameter space.  But
transformations that reflect changes in the data space are an outer product.  Transformations
from one basis for parameters to another uses this outer product for both its
column space and row space.  Kronecker operations give us just this result.

# Less-Than Full Factorial Models
An outer product with terms zeroed out.

In practice, many models with interaction or polynomial terms are not full factorial
models.  To derive the correct recentering and rescaling matrices means realizing
that some of the elements of $\beta$ are $0$.

## Dropping Higher Order Terms

Consider again the additive model
$$y = \beta_0 + \beta_1x_1 + \beta_2x_2$$

The recentering matrix for $x_1$ and $x_2$ is as given above, however we have
$$\begin{bmatrix}\beta_0^\delta \\ \beta_1^\delta \\ \beta_2^\delta \\ \beta_3^\delta \end{bmatrix}=
\begin{bmatrix} 1 & \mu_1 &\mu_2 &\mu_2\mu_1 \\
  0 &1 &0 &\mu_2 \\ 0 &0 &1 &\mu_1 \\ 0 &0 &0 &1 \end{bmatrix}
\begin{bmatrix}\beta_0 \\ \beta_1 \\ \beta_2 \\ 0 \end{bmatrix}$$
This simplifies to
$$\begin{bmatrix}\beta_0^\delta \\ \beta_1^\delta \\ \beta_2^\delta  \\ \beta_3^\delta \end{bmatrix}=
\begin{bmatrix} 1 & \mu_1 &\mu_2 \\
  0 &1 &0 \\ 0 &0 &1 \\ 0 &0 &0 \end{bmatrix}
\begin{bmatrix}\beta_0 \\ \beta_1 \\ \beta_2 \end{bmatrix}$$
Not surprisingly, this leaves $\beta_3^\delta=0$, and we can further simplify
$$\begin{bmatrix}\beta_0^\delta \\ \beta_1^\delta \\ \beta_2^\delta \end{bmatrix}=
\begin{bmatrix} 1 & \mu_1 &\mu_2 \\
  0 &1 &0 \\ 0 &0 &1 \end{bmatrix}
\begin{bmatrix}\beta_0 \\ \beta_1 \\ \beta_2 \end{bmatrix}$$
In other words, we end up with the only tranformation being to $\beta_0$, which 
had to be the result for a recentered additive model.  Following this approach,
we can derive our classic standardization formula for additive models as well.

## Dropping Lower Order Terms
Another point worth considering is the effect of recentering variables on a model
where a lower-order term has been dropped beneath a higher-order term.  Consider
the model
$$y = \beta_0 + \beta_1x_1 + \beta_3x_1x_2$$
where the term $\beta_2x_2$ has been dropped, setting $\beta_2=0$.

Here, our coefficient transformation looks like
$$\begin{bmatrix}\beta_0^\delta \\ \beta_1^\delta \\ \beta_2^\delta \\ \beta_3^\delta \end{bmatrix}=
\begin{bmatrix} 1 & \mu_1 &\mu_2 &\mu_2\mu_1 \\
  0 &1 &0 &\mu_2 \\ 0 &0 &1 &\mu_1 \\ 0 &0 &0 &1 \end{bmatrix}
\begin{bmatrix}\beta_0 \\ \beta_1 \\ 0 \\ \beta_3 \end{bmatrix}$$
We can simplify this somewhat as
$$\begin{bmatrix}\beta_0^\delta \\ \beta_1^\delta \\ \beta_2^\delta \\ \beta_3^\delta \end{bmatrix}=
\begin{bmatrix} 1 & \mu_1 &\mu_2\mu_1 \\
  0 &1 &\mu_2 \\ 0 &0 &\mu_1 \\ 0 &0 &1 \end{bmatrix}
\begin{bmatrix}\beta_0 \\ \beta_1 \\ \beta_3 \end{bmatrix}$$
But here we see that our recentered model gains a term and a coefficient, $\beta_2^\delta (=\mu_1\beta_3)$!

The highest order term in which a variable appears is always unchanged by recentering;
lower order terms change when any of the *other* variables in a higher order term
which includes the variables in the lower order term are recentered. 
Going back to the additive model, consisting of only first-order
(slope) and zero-order (intercept) terms, we see that only the intercept changes
when the first order $x_i$ are recentered.

If we build recentering and rescaling matrices variable by variable, we can use 
less-than-full factorial combinations as building blocks.  That is to say, we could
build a matrix for a full-factorial model and then drop columns for unused terms,
or we could approach this piecemeal.

[Checking for missing lower order terms in not currently implemented.  However, this
should be easy to accomplish:  drop columns in a full factorial $C$ or $S$ not included
in the coefficient vector (i.e. not among the terms), then check for rows that are
zero vectors and drop (only) those.]

## Untransformed Variables

It may be that the analyst wishes to leave some variables untransformed.  One way
to view this is that the recentering constant $\mu=0$ and the rescaling constant
$\sigma=1$.  So the "transformation" for this variable is just the identity matrix
$$S=C=\begin{bmatrix}1 & 0 \\ 0 &1 \end{bmatrix}$$
This leads to a simplification of the full factorial transformation matrix in terms
of direct sums.  If we have $C_2=I_2$, then
$$C_2 \otimes C_1 = \begin{bmatrix}C_1 &0 \\ 0 &C_1\end{bmatrix} = C_1 \oplus C_1 $$

# Polynomial Terms
An outer product with terms collected.

Like less-than-full factorial models, models with polynomial terms are worth a little
extra scrutiny.  Consider the model
$$y = \beta_0 + \beta_1x + \beta_3x^2$$
If we rewrite this as
$$y = \beta_0 + \beta_1x + \beta_2x +\beta_3xx$$
it looks like a factorial model. But all the effect of $x$ is
collected in a single term, $\beta_1$, so $\beta_2=0$.  This is perhaps easier to see
if we look at the recentering transformation
$$\begin{bmatrix}\beta_0^\delta \\ \beta_1^\delta \\ \beta_2^\delta \\ \beta_3^\delta \end{bmatrix}=
\begin{bmatrix} 1 & \mu &\mu &\mu\mu \\
  0 &1 &0 &\mu \\ 0 &0 &1 &\mu \\ 0 &0 &0 &1 \end{bmatrix}
\begin{bmatrix}\beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3  \end{bmatrix}$$
If we take $\beta_2=0$, then we can simplify as before
$$\begin{bmatrix}\beta_0^\delta \\ \beta_1^\delta \\ \beta_2^\delta \\ \beta_3^\delta \end{bmatrix}=
\begin{bmatrix} 1 & \mu &\mu\mu \\
  0 &1 &\mu \\ 0 &0 &\mu \\ 0 &0 &1 \end{bmatrix}
\begin{bmatrix}\beta_0 \\ \beta_1 \\ \beta_3  \end{bmatrix}$$
But now $\beta_1^\delta$ and $\beta_2^\delta$ both have part of the effect of a recentered $x_1$.
If we collect these terms we end up with
$$\begin{bmatrix}\beta_0^\delta \\ \beta_1^\delta \\ \beta_3^\delta \end{bmatrix}=
\begin{bmatrix} 1 & \mu &\mu\mu \\
  0 &1 &2\mu \\ 0 &0 &1 \end{bmatrix}
\begin{bmatrix}\beta_0 \\ \beta_1 \\ \beta_3  \end{bmatrix}$$
Models that include terms to any polynomial degree can be handled in this manner.

Polynomial models where higher degree terms are included while dropping lower degree terms,
when recentered, will have the lower order terms re-emerge, just as we saw with
less-than-full factorial models with dropped lower order terms.

# Categorical terms
In practice there are a number of approaches used for categorical variables, i.e. 
collections of indicator/contrast variables.

- Leaving the intercept terms in reference coding amounts to leaving the
coefficients for indicators untransformed, as described previously.
- Standardizing each term as a z-score is equivalent to treating each category in
the same manner as continuous variables, as described previously.
- Transforming to coding other than reference coding is again a "recentering" change of 
basis in that it changes where we find zero in the parameter space.
Here, however, we need another simple transformation.

For example, the general form of the grand-mean recentering matrix for a categorical
variable with $k$ categories is
$$
\begin{bmatrix}
1 &1/k     &\cdots &1/k \\
0 &(k-1)/k &\cdots &-1/k \\
0 &-1/k    &\cdots &-1/k \\
\vdots &\vdots &\vdots &\vdots \\
0 &-1/k    &\cdots &(k-1)/k
\end{bmatrix}
$$

This is then used in the same way as previously discussed recentering transformations.
(The first category remains the dropped column in this transformation.)

```{r}
C1 <- ref.to.gm(3)
rownames(C1) <- colnames(C1) <- c("(Intercept)", "cyl6", "cyl8")
C1
```

Combined with our transformation matrix for a continuous variable
```{r}
wtmean <- mean(mtcars$wt)
names(wtmean) <- "wt"
C2 <- mean.to.matrix(wtmean)
C <- kron(C2,C1)
C
```

This converts an uncentered model with reference coding for the indicators
to centered `wt` and grand-mean centered `cyl`.
```{r}
cylf <- as.factor(mtcars$cyl)
excat <- lm(mpg ~ cylf*wt, data=mtcars)
summary(excat)
```


```{r}
C %*% coef(excat)
```

```{r}
contrasts(cylf) <- contr.sum
coef(lm(mpg~cylf*wtcentered, data=mtcars)) # note different dropped level
```